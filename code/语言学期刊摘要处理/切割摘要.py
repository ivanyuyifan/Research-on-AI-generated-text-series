import os
from nltk.tokenize import sent_tokenize

# è¾“å…¥è¾“å‡ºè·¯å¾„
input_path = "/Users/fafaya/Desktop/Merge_all_abstracts.txt"
output_dir = "/Users/fafaya/Desktop/Abstracts_Split"
os.makedirs(output_dir, exist_ok=True)

# è¯»å–å¹¶æŒ‰å¥åˆ‡å‰²
with open(input_path, 'r', encoding='utf-8') as f:
    text = f.read()

sentences = sent_tokenize(text)
chunk_size = 10  # æ¯ 10 å¥ä¸ºä¸€ä¸ªæ–‡ä»¶

print(f"ğŸ“„ æ€»å¥æ•°: {len(sentences)}ï¼Œå°†ç”Ÿæˆ {len(sentences)//chunk_size + 1} ä¸ª chunk æ–‡ä»¶...")

# å†™å…¥ chunk æ–‡ä»¶
for i in range(0, len(sentences), chunk_size):
    chunk = ' '.join(sentences[i:i+chunk_size])
    filename = f"chunk_{i//chunk_size}.txt"
    with open(os.path.join(output_dir, filename), 'w', encoding='utf-8') as f_out:
        f_out.write(chunk)

print(f"âœ… æ‰€æœ‰ chunk æ–‡ä»¶å·²ä¿å­˜è‡³: {output_dir}")
